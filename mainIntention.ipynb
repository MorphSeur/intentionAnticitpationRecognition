{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a867b81-8d9c-4bd5-b4ad-6785791afaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "mainPath = \"D:\\\\Etude\\\\Doctorat\\\\Neural Network\\\\Environment\\\\yoloOpenCV\\\\\"\n",
    "\n",
    "# pathToVideo  = mainPath + \"input\\\\Robot_1FPS.mp4\"\n",
    "pathToVideo  = mainPath + \"input\\\\Egocentric_1FPS.mp4\"\n",
    "# pathToONNX   = mainPath + \"modelsONNX\\\\best4.onnx\"                # Robot Occluded objects\n",
    "pathToONNX   = mainPath + \"modelsONNX\\\\egocentric04022022.onnx\"\n",
    "# pathToONNX   = mainPath + \"modelsONNX\\\\robot04022022.onnx\"\n",
    "pathToLabels = mainPath + \"labels\"\n",
    "\n",
    "def build_model(is_cuda):\n",
    "    net = cv2.dnn.readNet(pathToONNX)\n",
    "    if is_cuda:\n",
    "        print(\"Attempty to use CUDA\")\n",
    "        net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "        net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA_FP16)\n",
    "    else:\n",
    "        print(\"Running on CPU\")\n",
    "        net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "        net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "    return net\n",
    "\n",
    "INPUT_WIDTH = 640\n",
    "INPUT_HEIGHT = 640\n",
    "# INPUT_WIDTH = 1920\n",
    "# INPUT_HEIGHT = 1080\n",
    "\n",
    "def detect(image, net):\n",
    "    blob = cv2.dnn.blobFromImage(image, 1/255.0, (640, 640), swapRB=True, crop=False)\n",
    "    # blob = cv2.dnn.blobFromImage(image, 1/255.0, (640, 640), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    preds = net.forward()\n",
    "    return preds\n",
    "\n",
    "def load_capture():\n",
    "    capture = cv2.VideoCapture(pathToVideo)\n",
    "    return capture\n",
    "\n",
    "def format_yolov5(frame):\n",
    "    row, col, _ = frame.shape\n",
    "    _max = max(col, row)\n",
    "    result = np.zeros((_max, _max, 3), np.uint8)\n",
    "    result[0:row, 0:col] = frame\n",
    "    return result\n",
    "\n",
    "labelsPath = os.path.sep.join([pathToLabels, \"labelsEgocentric.txt\"]) # \"labelsRobot.txt\"]) # \"labelsEgocentric.txt\"])\n",
    "LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
    "\n",
    "# initialize a list of colors to represent each possible class label\n",
    "np.random.seed(42)\n",
    "COLORS = np.random.randint(0, 255, size=(len(LABELS), 3), dtype=\"uint8\")\n",
    "\n",
    "is_cuda = len(sys.argv) > 1 and sys.argv[1] == \"cuda\"\n",
    "nett = build_model(is_cuda)\n",
    "\n",
    "def YOLO_OBJECTS(image, frame_w, frame_h, layerOutputs):\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    classIDs = []\n",
    "    labels = []\n",
    "    centPoints = []\n",
    "    ignoredClasses = ['phone','person','diningtable']\n",
    "    \n",
    "    output_data = layerOutputs[0]\n",
    "    \n",
    "    rows = output_data.shape[0]\n",
    "    \n",
    "    image_width, image_height, _ = image.shape\n",
    "\n",
    "    x_factor = image_width  / INPUT_WIDTH  #frame_w   # INPUT_WIDTH\n",
    "    y_factor = image_height / INPUT_WIDTH  #frame_h   # INPUT_HEIGHT\n",
    "    \n",
    "    for r in range(rows):\n",
    "        row = output_data[r]\n",
    "        confidence = row[4]\n",
    "        if confidence >= 0.5:\n",
    "            scores = row[5:]\n",
    "            _, _, _, max_indx = cv2.minMaxLoc(scores)\n",
    "            classID = max_indx[1]\n",
    "            if (scores[classID] > .25):\n",
    "                confidences.append(confidence)\n",
    "                classIDs.append(classID)\n",
    "\n",
    "                x, y, w, h = int(row[0].item()), int(row[1].item()), int(row[2].item()), int(row[3].item())\n",
    "                left = int((x - 0.5 * w) * x_factor)\n",
    "                top = int((y - 0.5 * h) * y_factor)\n",
    "                width = int(w * x_factor)\n",
    "                height = int(h * y_factor)\n",
    "                box = np.array([left, top, width, height])\n",
    "                boxes.append(box)\n",
    "\n",
    "    # apply non-maxima suppression to suppress weak, overlapping\n",
    "    # bounding boxes\n",
    "    conf = []\n",
    "    idxs = cv2.dnn.NMSBoxes(boxes, confidences, 0.25, 0.25)\n",
    "    # ensure at least one detection exists\n",
    "    if len(idxs) > 0:\n",
    "        # loop over the indexes we are keeping\n",
    "        for i in idxs.flatten():\n",
    "            # extract the bounding box coordinates\n",
    "            (x, y) = (boxes[i][0], boxes[i][1])\n",
    "            (w, h) = (boxes[i][2], boxes[i][3])\n",
    "            # draw a bounding box rectangle and label on the frame\n",
    "            if  LABELS[classIDs[i]] not in ignoredClasses :\n",
    "                centPoints.append((int(x+(w/2)) ,int(y+(h/2))))\n",
    "                # color = [int(c) for c in COLORS[classIDs[i]]]\n",
    "                # cv2.rectangle(image, (x, y), (x + w, y + h), color, 35)\n",
    "                # cv2.imwrite(os.path.join(pathOutput, '{}.jpg'.format(uuid.uuid1())), image)\n",
    "                labels.append(LABELS[classIDs[i]])\n",
    "                conf.append(format(confidences[i], \".2f\"))\n",
    "                \n",
    "    indexRHands = []\n",
    "    indexLHands = []\n",
    "    indexObject = []\n",
    "    for xx in range(len(labels)):\n",
    "        if labels[xx] == 'rhand':\n",
    "            indexRHands.append(xx) # print(\"Hooray!!\", xx)\n",
    "        elif labels[xx] == 'lhand':\n",
    "            indexLHands.append(xx) # print(\"Hooray!!\", xx)\n",
    "        elif labels[xx] != 'rhand' or labels[xx] != 'lhand':\n",
    "            indexObject.append(xx)\n",
    "    return (image, boxes, centPoints, conf, labels, indexRHands, indexLHands, indexObject)\n",
    "\n",
    "\n",
    "def draw_points (image, pnts, lbls, hands = False):\n",
    "    image_h, image_w, _ = image.shape\n",
    "    # print(\"Type image \", type(image))\n",
    "    # print(\"Type pnts \", type(pnts))\n",
    "    # print(\"Type lbls \", type(lbls))\n",
    "    for idx, pnt in enumerate(pnts):\n",
    "        color = []\n",
    "        if hands:\n",
    "            color = [0,255,0]\n",
    "        else:\n",
    "            color = [0,0,255]\n",
    "        cv2.circle(image, (pnt[0], pnt[1]), 8, color , -1 , 8)\n",
    "        cv2.line(image, (pnt[0] + 16, pnt[1] - 15), (pnt[0] + 50, pnt[1] - 30), [100, 10, 255], 2, 8)\n",
    "        cv2.putText(image, lbls[idx] , (pnt[0] + 50, pnt[1] - 30), cv2.FONT_HERSHEY_SIMPLEX, 1.5, [50, 200, 250], 2, cv2.LINE_AA)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9baaee5e-0126-4d62-818a-be071ed24606",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# Ontology Reasoning Functions\n",
    "################################################\n",
    "\n",
    "def load_data():\n",
    "    model = KeyedVectors.load_word2vec_format(mainPath + \"modelsONNX\\\\numberbatch-en.txt\")\n",
    "    return model\n",
    "\n",
    "def remove_duplicate(items):\n",
    "  return list(dict.fromkeys(items))\n",
    "\n",
    "\n",
    "def get_Affordances(model, totalObjects, objectsInAction, activities):\n",
    "    affordances = []\n",
    "    affordancesInAction = []\n",
    "\n",
    "    totalObjects = [obj.lower() for obj in totalObjects]\n",
    "    objectsInAction = [obj.lower() for obj in objectsInAction]\n",
    "\n",
    "    for obj in totalObjects :\n",
    "\n",
    "        affords = model.most_similar_to_given(obj,activities)\n",
    "        affordances.append(affords)\n",
    "        # print('Object : {0} affords : {1}'.format(obj,affords))\n",
    "\n",
    "    for obj in objectsInAction:\n",
    "        affordancesInAction.append(affordances[totalObjects.index(obj)])\n",
    "\n",
    "    affordances = remove_duplicate(affordances)\n",
    "    affordancesInAction = remove_duplicate(affordancesInAction)\n",
    "    return affordances,affordancesInAction\n",
    "\n",
    "def mapObjectsToConceptNet(objList):\n",
    "    ConceptNetMap = {\n",
    "        'pottedplant' : 'plant',\n",
    "        'diningtable' : 'table',\n",
    "        'tvmonitor' : 'tv',\n",
    "        'traffic light' : 'light',\n",
    "        'cell phone' : 'phone',\n",
    "        'right hand' : 'rhand',\n",
    "        'left hand' : 'lhand'\n",
    "    }\n",
    "\n",
    "    for idx, obj in enumerate(objList):\n",
    "        if obj in ConceptNetMap.keys():\n",
    "            objList[idx] = ConceptNetMap[obj]\n",
    "    return objList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7232da10-3db4-4b87-a40c-9088265a0ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConceptNet Model Loaded ....\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from scipy import spatial\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.downloader import base_dir\n",
    "\n",
    "# Define Main Activities\n",
    "activities = ['eat','drink','rest','socialize','work','write']\n",
    "\n",
    "ConceptNetmodel = load_data()\n",
    "print (\"ConceptNet Model Loaded ....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "865f76cc-a788-4797-ad1e-c7b513635d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "\n",
    "def buildAffordanceLeft(pLoc, pLocPr, cLoc, cLocPr, pOoi, pOoiPr, cOoi, cOoiPr):\n",
    "    # File One: Read the sorts and store in a .txt file.\n",
    "    pLocation = 'location(' + pLoc + ',' + ' ' + pLocPr + ').'\n",
    "    cLocation = 'location(' + cLoc + ',' + ' ' + cLocPr + ').'\n",
    "    pObjectOI = 'objectOI(' + pOoi + ',' + ' ' + pOoiPr + ').'\n",
    "    cObjectOI = 'objectOI(' + cOoi + ',' + ' ' + cOoiPr + ').'\n",
    "    \n",
    "    # print (pLocation)\n",
    "    # print (cLocation)\n",
    "    # print (pObjectOI)\n",
    "    # print (cObjectOI)\n",
    "    \n",
    "    with open('./lp/sortsLeft.lp', 'w') as out:\n",
    "        out.write('{}\\n{}\\n{}\\n{}\\n'.format(pLocation, cLocation, pObjectOI, cObjectOI))\n",
    "        # print ('\\n./lp/sortsLeft.lp is stored')\n",
    "    \n",
    "    sortsLeft = open(\"./lp/sortsLeft.lp\")\n",
    "    linesortsLeft = sortsLeft.readlines()\n",
    "        \n",
    "    # File Two: Read the sorts, build the events and store in a .txt file.\n",
    "    pEChanLoc = '\\nhappens(changeOOI(' + pOoi + ',' + ' ' + cOoi + '), 1).'\n",
    "    pEChanOOI = 'happens(changeLocation(' + pLoc + ',' + ' ' + cLoc + '), 1).'\n",
    "\n",
    "    # print (pEChanLoc)\n",
    "    # print (pEChanOOI)\n",
    "\n",
    "    csloi = \"not releasedAt(F,0) :- fluent(F).\"\n",
    "\n",
    "    with open('./lp/eventsLeft.lp', 'w') as out:\n",
    "        out.write('{}\\n{}\\n{}\\n'.format(csloi, pEChanLoc, pEChanOOI))\n",
    "        # print ('\\n./lp/eventsLeft.lp is stored')\n",
    "    \n",
    "    eventsLeft = open(\"./lp/eventsLeft.lp\")\n",
    "    lineeventsLeft = eventsLeft.readlines()\n",
    "    \n",
    "    command = \"clingo\\\\clingoWindows.exe -c maxtime=3 -n 1 \" +\\\n",
    "              \"lp\\\\decOrigin2.lp \" +\\\n",
    "              \"lp\\\\sortsLeft.lp \" +\\\n",
    "              \"lp\\\\eventsLeft.lp \" +\\\n",
    "              \"lp\\\\activities.lp \" +\\\n",
    "              \"lp\\\\ecasp888.lp \" +\\\n",
    "              \"lp\\\\ecasp88888.lp | \" +\\\n",
    "              \"clingo\\\\format-output.exe 3 > \" +\\\n",
    "              \"inference\\\\ecasp88Left.txt\"\n",
    "    \n",
    "    # command = \"./clingo/clingoLinux -c maxtime=3 -n 1 ./lp/decOrigin2.lp ./lp/sorts.lp ./lp/events.lp ./lp/activities.lp ./lp/ecasp888.lp ./lp/ecasp88888.lp | ./clingo/format-output 3 > ./inference/ecasp88.txt\"\n",
    "\n",
    "    os.system(command)\n",
    "    \n",
    "    intention = open(mainPath + \"inference\\\\ecasp88Left.txt\")\n",
    "    line = intention.readlines()\n",
    "    intention = re.search('\\+intention\\((.+?),', line[15])\n",
    "    if intention:\n",
    "        foundIntenLeft = intention.group(1)\n",
    "        foundConfLeft = line[15][-4:-2]\n",
    "    else:\n",
    "        foundIntenLeft = []\n",
    "        foundConfLeft = []\n",
    "    # print(\"\\nfoundIntenLeft:\", foundIntenLeft)\n",
    "    # print(\"foundConf:\", foundConfLeft)\n",
    "    return foundIntenLeft, foundConfLeft\n",
    "\n",
    "def buildAffordanceRight(pLoc, pLocPr, cLoc, cLocPr, pOoi, pOoiPr, cOoi, cOoiPr):\n",
    "    # File One: Read the sorts and store in a .txt file.\n",
    "    pLocation = 'location(' + pLoc + ',' + ' ' + pLocPr + ').'\n",
    "    cLocation = 'location(' + cLoc + ',' + ' ' + cLocPr + ').'\n",
    "    pObjectOI = 'objectOI(' + pOoi + ',' + ' ' + pOoiPr + ').'\n",
    "    cObjectOI = 'objectOI(' + cOoi + ',' + ' ' + cOoiPr + ').'\n",
    "    \n",
    "    # print (pLocation)\n",
    "    # print (cLocation)\n",
    "    # print (pObjectOI)\n",
    "    # print (cObjectOI)\n",
    "    \n",
    "    with open('./lp/sortsRight.lp', 'w') as out:\n",
    "        out.write('{}\\n{}\\n{}\\n{}\\n'.format(pLocation, cLocation, pObjectOI, cObjectOI))\n",
    "        # print ('\\n./lp/sortsRight.lp is stored')\n",
    "        \n",
    "    # File Two: Read the sorts, build the events and store in a .txt file.\n",
    "    pEChanLoc = '\\nhappens(changeOOI(' + pOoi + ',' + ' ' + cOoi + '), 1).'\n",
    "    pEChanOOI = 'happens(changeLocation(' + pLoc + ',' + ' ' + cLoc + '), 1).'\n",
    "\n",
    "    # print (pEChanLoc)\n",
    "    # print (pEChanOOI)\n",
    "\n",
    "    csloi = \"not releasedAt(F,0) :- fluent(F).\"\n",
    "\n",
    "    with open('./lp/eventsRight.lp', 'w') as out:\n",
    "        out.write('{}\\n{}\\n{}\\n'.format(csloi, pEChanLoc, pEChanOOI))\n",
    "        # print ('\\n./lp/eventsRight.lp is stored')\n",
    "    \n",
    "    command = \"clingo\\\\clingoWindows.exe -c maxtime=3 -n 1 \" + \\\n",
    "              \"lp\\\\decOrigin2.lp \" +\\\n",
    "              \"lp\\\\sortsRight.lp \" +\\\n",
    "              \"lp\\\\eventsRight.lp \" +\\\n",
    "              \"lp\\\\activities.lp \" +\\\n",
    "              \"lp\\\\ecasp888.lp \" +\\\n",
    "              \"lp\\\\ecasp88888.lp | \" +\\\n",
    "              \"clingo\\\\format-output.exe 3 > \" +\\\n",
    "              \"inference\\\\ecasp88Right.txt\"\n",
    "    # command = \"./clingo/clingoLinux -c maxtime=3 -n 1 ./lp/decOrigin2.lp ./lp/sorts.lp ./lp/events.lp ./lp/activities.lp ./lp/ecasp888.lp ./lp/ecasp88888.lp | ./clingo/format-output 3 > ./inference/ecasp88.txt\"\n",
    "\n",
    "    os.system(command)\n",
    "    \n",
    "    intention = open(mainPath + \"inference\\\\ecasp88Right.txt\")\n",
    "    line = intention.readlines()\n",
    "    intention = re.search('\\+intention\\((.+?),', line[15])\n",
    "    if intention:\n",
    "        foundIntenRight = intention.group(1)\n",
    "        foundConfRight = line[15][-4:-2]\n",
    "    else:\n",
    "        foundIntenRight = []\n",
    "        foundConfRight = []\n",
    "    # print(\"\\nfoundIntenRight:\", foundIntenRight)\n",
    "    # print(\"foundConfRight:\", foundConfRight)\n",
    "    return foundIntenRight, foundConfRight\n",
    "\n",
    "def sceneDetection():\n",
    "    x = random.uniform(0.65,0.95)\n",
    "    x = format(x, \".2f\")\n",
    "    x = str(x)\n",
    "    x = x[2:4]\n",
    "    # print(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71f29fa6-8c21-4502-915b-e03635e5d175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 83/83 [02:32<00:00,  1.84s/it]\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "#   Predict bounding boxes\n",
    "###############################\n",
    "\n",
    "video_reader = cv2.VideoCapture(pathToVideo)\n",
    "\n",
    "nb_frames = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "frame_h   = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_w   = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "\n",
    "#INPUT_WIDTH = frame_w\n",
    "#INPUT_HEIGHT = frame_h\n",
    "\n",
    "video_out = pathToVideo[:-4] + '_detected' + pathToVideo[-4:]\n",
    "\n",
    "video_writer = cv2.VideoWriter(video_out,\n",
    "                       cv2.VideoWriter_fourcc(*'XVID'), \n",
    "                       50.0,\n",
    "                       (frame_h, frame_h))\n",
    "\n",
    "cv2.namedWindow(\"Image\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "objectsOfIntrestLEC = []\n",
    "objectsOfIntrestLECConf = []\n",
    "objectsOfIntrestREC = []\n",
    "objectsOfIntrestRECConf = []\n",
    "\n",
    "foundIntenLeft = \"init\"\n",
    "foundConfLeft = \"init\"\n",
    "foundIntenRight = \"init\"\n",
    "foundConfRight = \"init\"\n",
    "\n",
    "for i in tqdm(range(nb_frames)):\n",
    "    # Read Image\n",
    "    _, image = video_reader.read()\n",
    "    # blank_image = np.zeros((frame_h,frame_w,3), np.uint8)\n",
    "    # Recognize Objects\n",
    "    \n",
    "    image = cv2.resize(image, (1080, 1080))\n",
    "    \n",
    "    image = format_yolov5(image)\n",
    "    \n",
    "    layerOutputs = detect(image, nett)\n",
    "    \n",
    "    image, OBJboxes, OBJcentPoints, confidences, labels, indexRHands, indexLHands, \\\n",
    "                indexObject = YOLO_OBJECTS(image, frame_w, frame_h, layerOutputs)\n",
    "    \n",
    "    # Recognize Hands\n",
    "    OBJECTcentPoints = []\n",
    "    OBJECTlabels     = []\n",
    "    confObjects      = []\n",
    "    RHANDcentPoints  = []\n",
    "    RHANDlabels      = []\n",
    "    confRHand        = []\n",
    "    LHANDcentPoints  = []\n",
    "    LHANDlabels      = []\n",
    "    confLHand        = []\n",
    "    \n",
    "    for oo in range(len(indexObject)):\n",
    "        OBJECTcentPoints.append(OBJcentPoints[indexObject[oo]])\n",
    "        OBJECTlabels.append(labels[indexObject[oo]])\n",
    "        confObjects.append(confidences[indexObject[oo]])\n",
    "        \n",
    "    for rr in range(len(indexRHands)):\n",
    "        RHANDcentPoints.append(OBJcentPoints[indexRHands[rr]])\n",
    "        RHANDlabels.append(labels[indexRHands[rr]])\n",
    "        confRHand.append(confidences[indexRHands[rr]])\n",
    "        \n",
    "    for ll in range(len(indexLHands)):\n",
    "        LHANDcentPoints.append(OBJcentPoints[indexLHands[ll]])\n",
    "        LHANDlabels.append(labels[indexLHands[ll]])\n",
    "        confLHand.append(confidences[indexLHands[ll]])\n",
    "        \n",
    "    OBJECTcentPoints = list(OBJECTcentPoints)\n",
    "    RHANDcentPoints  = list(RHANDcentPoints)\n",
    "    LHANDcentPoints  = list(LHANDcentPoints)\n",
    "    \n",
    "    if len(OBJECTcentPoints) > 0:\n",
    "        image = draw_points(image, OBJECTcentPoints[0:8], OBJECTlabels[0:8], hands=False)\n",
    "        \n",
    "    if len(LHANDcentPoints) > 0:\n",
    "        image = draw_points(image, LHANDcentPoints, LHANDlabels, hands=True)\n",
    "        \n",
    "    if len(RHANDcentPoints) > 0:\n",
    "        image = draw_points(image, RHANDcentPoints, RHANDlabels, hands=True)\n",
    "\n",
    "    # Calculate nearest neigbour\n",
    "    objectsOfIntrestR = []\n",
    "    objectsOfIntrestL = []\n",
    "    \n",
    "    if len(OBJECTcentPoints) > 0:\n",
    "        tree = spatial.KDTree(OBJECTcentPoints)\n",
    "        for lhand in LHANDcentPoints :\n",
    "            distance, LnearestOBJidx = tree.query(lhand)\n",
    "            LnearestOBJidx = int(LnearestOBJidx)\n",
    "            l = tuple(tree.data[LnearestOBJidx])\n",
    "            idxl = OBJECTcentPoints.index(tuple(tree.data[LnearestOBJidx]))\n",
    "            objectsOfIntrestL.append(OBJECTlabels[idxl])\n",
    "            \"\"\"print(\"\\nconfObjects[idxl]\", confObjects[idxl])\n",
    "            print(\"confObjects[idxl] type\", type(confObjects[idxl]))\n",
    "            print(\"len(confObjects[idxl])\", len(confObjects[idxl]))\n",
    "            if confObjects[idxl] != []:\n",
    "                objectsOfIntrestLECConf.append(confObjects[idxl])\n",
    "            elif objectsOfIntrestL == []:\n",
    "                objectsOfIntrestLECConf = objectsOfIntrestLECConf\"\"\"\n",
    "            cv2.line(image, lhand, (int(l[0]),int(l[1])), (255,0,0), 5)\n",
    "            \n",
    "        for rhand in RHANDcentPoints:\n",
    "            distance, RnearestOBJidx = tree.query(rhand)\n",
    "            RnearestOBJidx = int(RnearestOBJidx)\n",
    "            r = tuple(tree.data[RnearestOBJidx])\n",
    "            idxr = OBJECTcentPoints.index(tuple(tree.data[RnearestOBJidx]))\n",
    "            objectsOfIntrestR.append(OBJECTlabels[idxr])\n",
    "            \"\"\"print(\"\\nconfObjects[idxl]\", confObjects[idxl])\n",
    "            print(\"confObjects[idxl] type\", type(confObjects[idxl]))\n",
    "            print(\"len(confObjects[idxl])\", len(confObjects[idxl]))\n",
    "            if confObjects[idxl] != []:\n",
    "                objectsOfIntrestRECConf.append(confObjects[idxl])\n",
    "            elif objectsOfIntrestL == []:\n",
    "                objectsOfIntrestRECConf = objectsOfIntrestRECConf\"\"\"\n",
    "            cv2.line(image, rhand, (int(r[0]),int(r[1])), (255,0,0), 5)\n",
    "            \n",
    "    if len(OBJECTlabels) > 0:\n",
    "        totalObjects = mapObjectsToConceptNet(OBJECTlabels)\n",
    "    else:\n",
    "        totalObjects = []\n",
    "\n",
    "    cv2.putText(image, 'Objs: {}'.format(totalObjects[0:8]) , (15,30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, [0, 255, 0], 2, cv2.LINE_AA)\n",
    "    # cv2.putText(image, 'Objs Conf.: {}'.format(confObjects[0:7]) , (25,65), cv2.FONT_HERSHEY_SIMPLEX, 0.8, [0, 255, 0], 2, cv2.LINE_AA)\n",
    "    \n",
    "    if len(objectsOfIntrestR) > 0:\n",
    "        objectsInActionR = mapObjectsToConceptNet(objectsOfIntrestR)\n",
    "    else:\n",
    "        objectsInActionR = []\n",
    "        \n",
    "    if len(objectsOfIntrestL) > 0:\n",
    "        objectsInActionL = mapObjectsToConceptNet(objectsOfIntrestL)\n",
    "    else:\n",
    "        objectsInActionL = []\n",
    "\n",
    "    cv2.putText(image, 'Objs Int: LHand {} - RHand {}'.format(objectsInActionL, objectsInActionR) , (25,65), cv2.FONT_HERSHEY_SIMPLEX, 0.8, [0, 255, 0],2,cv2.LINE_AA)\n",
    "    # cv2.putText(image, 'Objs Int: LHand {}{} - RHand {}{}'.format(objectsOfIntrestL, confLHand, objectsOfIntrestR, confRHand) , (15,65), cv2.FONT_HERSHEY_SIMPLEX, 0.8, [0, 255, 0],2,cv2.LINE_AA)\n",
    "    # cv2.putText(image, 'Objs Int Conf.: LHand {} - RHand {}'.format(confLHand, confRHand) , (25,100), cv2.FONT_HERSHEY_SIMPLEX, 0.8, [0, 255, 0],2,cv2.LINE_AA)\n",
    "        \n",
    "    if objectsOfIntrestL != []:\n",
    "        objectsOfIntrestLEC.append(objectsOfIntrestL[0])\n",
    "    elif objectsOfIntrestL == []:\n",
    "        objectsOfIntrestLEC = objectsOfIntrestLEC\n",
    "        \n",
    "    if objectsOfIntrestR != []:\n",
    "        objectsOfIntrestREC.append(objectsOfIntrestR[0])\n",
    "    elif objectsOfIntrestR == []:\n",
    "        objectsOfIntrestREC = objectsOfIntrestREC\n",
    "    \n",
    "    pLoc   = 'office'\n",
    "    pLocPr = sceneDetection()\n",
    "    cLoc   = 'desktop'\n",
    "    cLocPr = sceneDetection()\n",
    "    \n",
    "    if len(objectsOfIntrestLEC) > 2:\n",
    "        if objectsOfIntrestLEC[-1] == objectsOfIntrestLEC[-2]:\n",
    "            objectsOfIntrestLEC[-1] = objectsOfIntrestLEC[-1] + '2'\n",
    "        pOoiLeft   = objectsOfIntrestLEC[-2]\n",
    "        pOoiPrLeft = confidences[-2]\n",
    "        pOoiPrLeft = str(pOoiPrLeft)\n",
    "        pOoiPrLeft = pOoiPrLeft[2:4]\n",
    "        cOoiLeft   = objectsOfIntrestLEC[-1]\n",
    "        cOoiPrLeft = confidences[-2]\n",
    "        cOoiPrLeft = str(cOoiPrLeft)\n",
    "        cOoiPrLeft = cOoiPrLeft[2:4]\n",
    "        # print(\"\\nobjectsOfIntrestLEC[-1]\", objectsOfIntrestLEC[-1])\n",
    "        # print(\"objectsOfIntrestLEC[-2]\", objectsOfIntrestLEC[-2])\n",
    "        foundIntenLeft, foundConfLeft = buildAffordanceLeft(pLoc, pLocPr, cLoc, cLocPr, pOoiLeft, pOoiPrLeft, cOoiLeft, cOoiPrLeft)\n",
    "    \n",
    "    if len(objectsOfIntrestREC) > 2:\n",
    "        if objectsOfIntrestREC[-1] == objectsOfIntrestREC[-2]:\n",
    "            objectsOfIntrestREC[-1] = objectsOfIntrestREC[-1] + '2'\n",
    "        # print(\"\\nobjectsOfIntrestREC[-1]\", objectsOfIntrestREC[-1])\n",
    "        # print(\"objectsOfIntrestREC[-2]\", objectsOfIntrestREC[-2])\n",
    "        pOoiRight   = objectsOfIntrestREC[-2]\n",
    "        pOoiPrRight = confidences[-2]\n",
    "        pOoiPrRight = str(pOoiPrRight)\n",
    "        pOoiPrRight = pOoiPrRight[2:4]\n",
    "        cOoiRight   = objectsOfIntrestREC[-1]\n",
    "        cOoiPrRight = confidences[-1]\n",
    "        cOoiPrRight = str(cOoiPrRight)\n",
    "        cOoiPrRight = cOoiPrRight[2:4]\n",
    "        foundIntenRight, foundConfRight = buildAffordanceRight(pLoc, pLocPr, cLoc, cLocPr, pOoiRight, pOoiPrRight, cOoiRight, cOoiPrRight)\n",
    "        \n",
    "    try:\n",
    "        affordancesR , affInActionR = get_Affordances(ConceptNetmodel,totalObjects,objectsInActionR,activities)\n",
    "        affordancesL , affInActionL = get_Affordances(ConceptNetmodel,totalObjects,objectsInActionL,activities)\n",
    "    except Exception as e:\n",
    "        print(\"errrerere\", e)\n",
    "        affordancesR , affInActionR = [], []\n",
    "        affordancesL , affInActionL = [], []\n",
    "    # cv2.putText(blank_image, 'Affordances : {0}'.format(affordances) , (50,300), cv2.FONT_HERSHEY_SIMPLEX, 2, [255, 0, 0],3,cv2.LINE_AA)\n",
    "    cv2.putText(image, 'Affordances: LHand {} - RHand {}'.format(affordancesL[0:3], affordancesR[0:3]) , (15,100), cv2.FONT_HERSHEY_SIMPLEX, 0.8, [255, 0, 0],2,cv2.LINE_AA)\n",
    "    # cv2.putText(blank_image, 'Activities : {0}'.format(affInAction) , (50,400), cv2.FONT_HERSHEY_SIMPLEX, 2, [0, 0, 255],3,cv2.LINE_AA)\n",
    "    # cv2.putText(image, 'Activities:  LHand {} - RHand {}'.format(affInActionL, affInActionR) , (25,170), cv2.FONT_HERSHEY_SIMPLEX, 0.8, [0, 0, 255],2,cv2.LINE_AA)\n",
    "    \n",
    "    cv2.putText(image, 'Activities: LHand {} {} - RHand {} {}'.format(\"['\"+foundIntenLeft+\",\",foundConfLeft+\"']\",\\\n",
    "                                                                      \"['\"+foundIntenRight+\",\",foundConfRight+\"']\"),\\\n",
    "                                                        (15,135), cv2.FONT_HERSHEY_SIMPLEX, 0.8, [0, 0, 255],2,cv2.LINE_AA)\n",
    "    \n",
    "    cv2.imshow('Image', image)\n",
    "    \n",
    "    # Write Video Frames\n",
    "    # pathOutputFramesRobot = mainPath + \"outputFramesRobot\"\n",
    "    pathOutputFramesRobot = mainPath + \"outputFramesEgocentric\"\n",
    "    # vis = np.concatenate((image,blank_image), axis=0)\n",
    "    # vis = cv2.resize(vis,None,fx=0.5,fy=0.5)\n",
    "    key = cv2.waitKey(30)\n",
    "    cv2.imwrite(os.path.join(pathOutputFramesRobot, '{}.jpg'.format(uuid.uuid1())), image)\n",
    "    video_writer.write(np.uint8(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5141e68-4108-4112-8b32-ec21365a313c",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_reader.release()\n",
    "video_writer.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb222ae8-305e-4ba4-aa0b-184f7d19e92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize video\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "pathToResize = mainPath + \"input\\\\\"\n",
    "\n",
    "# cap = cv2.VideoCapture(pathToResize + \"Robot_1FPS_detected.mp4\")\n",
    "cap = cv2.VideoCapture(pathToResize + \"Egocentric_1FPS_detected.mp4\")\n",
    " \n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "# out = cv2.VideoWriter(pathToResize + \"Robot_1FPS_detectedResized.mp4\",fourcc, 3.0, (1920,1080))\n",
    "out = cv2.VideoWriter(pathToResize + \"Egocentric_1FPS_detectedResized.mp4\",fourcc, 3.0, (1920,1080))\n",
    " \n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        b = cv2.resize(frame,(1920,1080))\n",
    "        out.write(b)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5bca25-d88a-4a56-865e-d37bf44786c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import fileinput\n",
    "\n",
    "def replacement(file):\n",
    "    for line in fileinput.input(file, inplace=1, backup='.bak'):\n",
    "        if \"0 \" in line[0:2]:\n",
    "            line = line[0:2].replace(\"0 \",\"80 \") + line[2:]\n",
    "        elif \"1 \" in line[0:2]:\n",
    "            line = line[0:2].replace(\"1 \",\"81 \") + line[2:]\n",
    "        elif \"2 \" in line[0:2]:\n",
    "            line = line[0:2].replace(\"2 \",\"82 \") + line[2:]\n",
    "        elif \"3 \" in line[0:2]:\n",
    "            line = line[0:2].replace(\"3 \",\"83 \") + line[2:]\n",
    "        elif \"4 \" in line[0:2]:\n",
    "            line = line[0:2].replace(\"4 \",\"84 \") + line[2:]\n",
    "        elif \"5 \" in line[0:2]:\n",
    "            line = line[0:2].replace(\"5 \",\"85 \") + line[2:]\n",
    "        elif \"6 \" in line[0:2]:\n",
    "            line = line[0:2].replace(\"6 \",\"86 \") + line[2:]\n",
    "        elif \"7 \" in line[0:2]:\n",
    "            line = line[0:2].replace(\"7 \",\"87 \") + line[2:]\n",
    "        sys.stdout.write(line)\n",
    "    fileinput.close()\n",
    "\n",
    "for i in range(54):\n",
    "    file = \"D:\\\\Etude\\\\Doctorat\\\\VirtualMachines\\\\sharedFolder\\\\ModifiedOpenLabelling-main2\\\\bbox_txt\\\\frame\" + str(i) + \".txt\"\n",
    "    replacement(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3853f19a-8e3b-4a6b-b1f1-99ac6e4856bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce FPS\n",
    "\n",
    "import cv2\n",
    "\n",
    "path = \"D:\\\\Etude\\\\Doctorat\\\\Inspiration Anticipation-Prediction\\\\Code\\\\input\\\\Egocentric.mp4\"\n",
    "pathOutputFrames = \"D:\\\\Etude\\\\Doctorat\\\\Inspiration Anticipation-Prediction\\\\Code\\\\yolov5-master\\\\yoloOpenCV\\\\outputFramesEgocentric\\\\frame\"\n",
    "\n",
    "vidcap = cv2.VideoCapture(path)\n",
    "\n",
    "def getFrame(sec):\n",
    "    vidcap.set(cv2.CAP_PROP_POS_MSEC,sec*1000) \n",
    "    hasFrames,image = vidcap.read() \n",
    "    if hasFrames:\n",
    "        cv2.imwrite(pathOutputFrames + str(sec) + \".jpg\", image)\n",
    "    return hasFrames \n",
    "\n",
    "sec = 0\n",
    "frameRate = 1 #it will capture image in each 0.5 second\n",
    "success = getFrame(sec)\n",
    "while success:\n",
    "    sec = sec + frameRate \n",
    "    sec = round(sec, 2) \n",
    "    success = getFrame(sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ae158a-1199-4684-8191-e94ed81cd34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frames Building\n",
    "\n",
    "import cv2\n",
    "\n",
    "pathInputFrames = \"D:\\\\Etude\\\\Doctorat\\\\Inspiration Anticipation-Prediction\\\\Code\\\\yolov5-master\\\\yoloOpenCV\\\\outputFrames\\\\frame\"\n",
    "pathOutputVideo = \"D:\\\\Etude\\\\Doctorat\\\\Inspiration Anticipation-Prediction\\\\Code\\\\input\\\\Robot_1FPS.mp4\"\n",
    "\n",
    "img_array = []\n",
    "\n",
    "def toCheck(frame):\n",
    "    x = 0\n",
    "    if frame is None:\n",
    "        return None\n",
    "\n",
    "def openFrame(x):\n",
    "    path = pathInputFrames + str(x) + \".jpg\"\n",
    "    frame = cv2.imread(path)\n",
    "    return frame\n",
    "    \n",
    "for x in range(1998):\n",
    "    frame = openFrame(x)\n",
    "    if frame is None:\n",
    "        x += 1\n",
    "        toCheck(frame)\n",
    "    else:   \n",
    "        height, width, layers = frame.shape\n",
    "        size = (width, height)\n",
    "        img_array.append(frame)\n",
    "\n",
    "        output = cv2.VideoWriter(pathOutputVideo, cv2.VideoWriter_fourcc(*'DIVX'), 15, size)\n",
    "\n",
    "for i in range(len(img_array)):\n",
    "    output.write(img_array[i])\n",
    "output.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
